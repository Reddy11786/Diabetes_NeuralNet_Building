{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXpvD-fXci6G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dataset \"Diabetes\""
      ],
      "metadata": {
        "id": "Q80o_3rsfq-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"https://raw.githubusercontent.com/fawazsammani/The-Complete-Neural-Networks-Bootcamp-Theory-Applications/master/diabetes.csv\")"
      ],
      "metadata": {
        "id": "ie5f1g9hcohw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As data is in pandas format so we will use iloc for arrray indexing in numpy format  "
      ],
      "metadata": {
        "id": "rUiJZK6L25Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=df.iloc[:,0:-1].values #features(all columns except the class column)\n",
        "y_string=list(df.iloc[:,-1]) #label\n",
        "y_string\n",
        "\n"
      ],
      "metadata": {
        "id": "V5pa4mn4hHse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC6lL1iS5o1p",
        "outputId": "ce04fb41-6429-4291-d87a-4654a582d0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset y_string as string, we convert string to int so that NN understands\n",
        "y_int=[]\n",
        "for i in y_string:\n",
        "  if i == \"positive\":\n",
        "    y_int.append(1)\n",
        "  else:\n",
        "    y_int.append(0)"
      ],
      "metadata": {
        "id": "R7gi7m7T5-J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#covert to an array\n",
        "y = np.array(y_int, dtype = 'float64')\n"
      ],
      "metadata": {
        "id": "TdgI_k437OuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature normalization to (-1,1)\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(x)\n"
      ],
      "metadata": {
        "id": "OQu086Mw9nxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert array into torch tensors\n",
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y).unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "YaAJLnw1ApS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV1GImvWBEy3",
        "outputId": "dd23b33f-20fd-48dc-95d1-1f338a9c7edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([768, 7])\n",
            "torch.Size([768, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "  def __init__(self,x,y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)"
      ],
      "metadata": {
        "id": "L-k_agAPAU1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= Dataset(x,y)"
      ],
      "metadata": {
        "id": "nYYsmb5YB1JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSwiCbGHCAqW",
        "outputId": "9f41757a-124c-42fc-9b80-c836d5cc3b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load data into dataloader for batch processing and shuffling\n",
        "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "Ff8RYxC8DmgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"there are {} batchs in the dataset\".format(len(train_loader)))\n",
        "for (x,y) in train_loader:\n",
        "  print(\"for 1 interation(batch),there is:\")\n",
        "  print(\"data:\",x.shape)\n",
        "  print(\"labels:\",y.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLppCM9VE6xN",
        "outputId": "01cdaca3-c6db-43be-e31f-5fc619f6ee29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 24 batchs in the dataset\n",
            "for 1 interation(batch),there is:\n",
            "data: torch.Size([32, 7])\n",
            "labels: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#building NN\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, input_features, output_features):\n",
        "    super(Model,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_features,5)\n",
        "    self.fc2 = nn.Linear(5,4)\n",
        "    self.fc3 = nn.Linear(4,3)\n",
        "    self.fc4 = nn.Linear(3, output_features)\n",
        "    self.Sigmoid = nn.Sigmoid()\n",
        "    self.Tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.Tanh(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.Tanh(out)\n",
        "    out = self.fc3(out)\n",
        "    out = self.Tanh(out)\n",
        "    out = self.fc4(out)\n",
        "    out = self.Sigmoid(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "S1k44TuWGPIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a network\n",
        "net = Model(7,1)\n",
        "#loss function Binary Cross Entrophy loss\n",
        "criterion = torch.nn.BCELoss(size_average = True)\n",
        "#optimizer SGD\n",
        "optimizer = torch.optim.SGD(net.parameters(),lr = 0.1, momentum = 0.9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO5SXH9rOLeZ",
        "outputId": "b9d2c525-994b-4ce8-db8d-e5421501b0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training the network\n",
        "epochs = 200\n",
        "for epoch in range(200):\n",
        "  for inputs,labels in train_loader:\n",
        "    inputs = inputs.float()\n",
        "    labels = labels.float()\n",
        "    #forward propogation\n",
        "    outputs = net(inputs)\n",
        "    #loss \n",
        "    loss = criterion(outputs,labels)\n",
        "    #clear the gradient buffer\n",
        "    optimizer.zero_grad()\n",
        "    #back propogation\n",
        "    loss.backward()\n",
        "    #update weights\n",
        "    optimizer.step()\n",
        "\n",
        "  #accuracy calculations\n",
        "  outputs = (outputs>0.5).float()\n",
        "  accuracy = (outputs == labels).float().mean()\n",
        "  print(\"Epoch{}/{}, Loss: {:.3f}, Accuracy{:.3f}\".format(epoch+1, epochs, loss, accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfs_guJJPsgU",
        "outputId": "ea92b64a-ea77-4ba8-8785-29407dbc091e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1/200, Loss: 0.547, Accuracy0.719\n",
            "Epoch2/200, Loss: 0.469, Accuracy0.750\n",
            "Epoch3/200, Loss: 0.522, Accuracy0.719\n",
            "Epoch4/200, Loss: 0.565, Accuracy0.656\n",
            "Epoch5/200, Loss: 0.529, Accuracy0.719\n",
            "Epoch6/200, Loss: 0.512, Accuracy0.781\n",
            "Epoch7/200, Loss: 0.534, Accuracy0.750\n",
            "Epoch8/200, Loss: 0.432, Accuracy0.781\n",
            "Epoch9/200, Loss: 0.440, Accuracy0.875\n",
            "Epoch10/200, Loss: 0.542, Accuracy0.688\n",
            "Epoch11/200, Loss: 0.523, Accuracy0.781\n",
            "Epoch12/200, Loss: 0.645, Accuracy0.719\n",
            "Epoch13/200, Loss: 0.619, Accuracy0.594\n",
            "Epoch14/200, Loss: 0.438, Accuracy0.812\n",
            "Epoch15/200, Loss: 0.610, Accuracy0.781\n",
            "Epoch16/200, Loss: 0.424, Accuracy0.844\n",
            "Epoch17/200, Loss: 0.600, Accuracy0.719\n",
            "Epoch18/200, Loss: 0.439, Accuracy0.812\n",
            "Epoch19/200, Loss: 0.347, Accuracy0.938\n",
            "Epoch20/200, Loss: 0.381, Accuracy0.812\n",
            "Epoch21/200, Loss: 0.439, Accuracy0.750\n",
            "Epoch22/200, Loss: 0.313, Accuracy0.906\n",
            "Epoch23/200, Loss: 0.492, Accuracy0.750\n",
            "Epoch24/200, Loss: 0.538, Accuracy0.688\n",
            "Epoch25/200, Loss: 0.372, Accuracy0.906\n",
            "Epoch26/200, Loss: 0.569, Accuracy0.719\n",
            "Epoch27/200, Loss: 0.435, Accuracy0.781\n",
            "Epoch28/200, Loss: 0.466, Accuracy0.719\n",
            "Epoch29/200, Loss: 0.471, Accuracy0.812\n",
            "Epoch30/200, Loss: 0.397, Accuracy0.906\n",
            "Epoch31/200, Loss: 0.465, Accuracy0.781\n",
            "Epoch32/200, Loss: 0.337, Accuracy0.812\n",
            "Epoch33/200, Loss: 0.429, Accuracy0.875\n",
            "Epoch34/200, Loss: 0.329, Accuracy0.844\n",
            "Epoch35/200, Loss: 0.359, Accuracy0.875\n",
            "Epoch36/200, Loss: 0.452, Accuracy0.781\n",
            "Epoch37/200, Loss: 0.732, Accuracy0.594\n",
            "Epoch38/200, Loss: 0.487, Accuracy0.875\n",
            "Epoch39/200, Loss: 0.419, Accuracy0.844\n",
            "Epoch40/200, Loss: 0.359, Accuracy0.875\n",
            "Epoch41/200, Loss: 0.291, Accuracy0.875\n",
            "Epoch42/200, Loss: 0.526, Accuracy0.688\n",
            "Epoch43/200, Loss: 0.690, Accuracy0.594\n",
            "Epoch44/200, Loss: 0.508, Accuracy0.750\n",
            "Epoch45/200, Loss: 0.347, Accuracy0.812\n",
            "Epoch46/200, Loss: 0.408, Accuracy0.875\n",
            "Epoch47/200, Loss: 0.383, Accuracy0.875\n",
            "Epoch48/200, Loss: 0.439, Accuracy0.812\n",
            "Epoch49/200, Loss: 0.342, Accuracy0.906\n",
            "Epoch50/200, Loss: 0.431, Accuracy0.844\n",
            "Epoch51/200, Loss: 0.455, Accuracy0.781\n",
            "Epoch52/200, Loss: 0.528, Accuracy0.750\n",
            "Epoch53/200, Loss: 0.525, Accuracy0.750\n",
            "Epoch54/200, Loss: 0.342, Accuracy0.906\n",
            "Epoch55/200, Loss: 0.463, Accuracy0.781\n",
            "Epoch56/200, Loss: 0.317, Accuracy0.875\n",
            "Epoch57/200, Loss: 0.565, Accuracy0.781\n",
            "Epoch58/200, Loss: 0.598, Accuracy0.719\n",
            "Epoch59/200, Loss: 0.555, Accuracy0.719\n",
            "Epoch60/200, Loss: 0.376, Accuracy0.906\n",
            "Epoch61/200, Loss: 0.444, Accuracy0.781\n",
            "Epoch62/200, Loss: 0.499, Accuracy0.812\n",
            "Epoch63/200, Loss: 0.417, Accuracy0.875\n",
            "Epoch64/200, Loss: 0.383, Accuracy0.875\n",
            "Epoch65/200, Loss: 0.411, Accuracy0.812\n",
            "Epoch66/200, Loss: 0.636, Accuracy0.750\n",
            "Epoch67/200, Loss: 0.489, Accuracy0.844\n",
            "Epoch68/200, Loss: 0.466, Accuracy0.812\n",
            "Epoch69/200, Loss: 0.462, Accuracy0.875\n",
            "Epoch70/200, Loss: 0.461, Accuracy0.812\n",
            "Epoch71/200, Loss: 0.346, Accuracy0.844\n",
            "Epoch72/200, Loss: 0.631, Accuracy0.688\n",
            "Epoch73/200, Loss: 0.399, Accuracy0.812\n",
            "Epoch74/200, Loss: 0.472, Accuracy0.719\n",
            "Epoch75/200, Loss: 0.326, Accuracy0.875\n",
            "Epoch76/200, Loss: 0.348, Accuracy0.844\n",
            "Epoch77/200, Loss: 0.352, Accuracy0.844\n",
            "Epoch78/200, Loss: 0.560, Accuracy0.812\n",
            "Epoch79/200, Loss: 0.570, Accuracy0.750\n",
            "Epoch80/200, Loss: 0.461, Accuracy0.781\n",
            "Epoch81/200, Loss: 0.366, Accuracy0.875\n",
            "Epoch82/200, Loss: 0.356, Accuracy0.875\n",
            "Epoch83/200, Loss: 0.548, Accuracy0.781\n",
            "Epoch84/200, Loss: 0.392, Accuracy0.844\n",
            "Epoch85/200, Loss: 0.543, Accuracy0.781\n",
            "Epoch86/200, Loss: 0.339, Accuracy0.875\n",
            "Epoch87/200, Loss: 0.495, Accuracy0.812\n",
            "Epoch88/200, Loss: 0.396, Accuracy0.844\n",
            "Epoch89/200, Loss: 0.571, Accuracy0.781\n",
            "Epoch90/200, Loss: 0.418, Accuracy0.875\n",
            "Epoch91/200, Loss: 0.377, Accuracy0.875\n",
            "Epoch92/200, Loss: 0.581, Accuracy0.719\n",
            "Epoch93/200, Loss: 0.270, Accuracy0.906\n",
            "Epoch94/200, Loss: 0.238, Accuracy0.938\n",
            "Epoch95/200, Loss: 0.579, Accuracy0.812\n",
            "Epoch96/200, Loss: 0.358, Accuracy0.875\n",
            "Epoch97/200, Loss: 0.388, Accuracy0.875\n",
            "Epoch98/200, Loss: 0.538, Accuracy0.750\n",
            "Epoch99/200, Loss: 0.369, Accuracy0.875\n",
            "Epoch100/200, Loss: 0.215, Accuracy0.906\n",
            "Epoch101/200, Loss: 0.633, Accuracy0.688\n",
            "Epoch102/200, Loss: 0.286, Accuracy0.906\n",
            "Epoch103/200, Loss: 0.313, Accuracy0.875\n",
            "Epoch104/200, Loss: 0.310, Accuracy0.875\n",
            "Epoch105/200, Loss: 0.363, Accuracy0.844\n",
            "Epoch106/200, Loss: 0.562, Accuracy0.719\n",
            "Epoch107/200, Loss: 0.354, Accuracy0.844\n",
            "Epoch108/200, Loss: 0.876, Accuracy0.562\n",
            "Epoch109/200, Loss: 0.425, Accuracy0.844\n",
            "Epoch110/200, Loss: 0.298, Accuracy0.906\n",
            "Epoch111/200, Loss: 0.432, Accuracy0.812\n",
            "Epoch112/200, Loss: 0.290, Accuracy0.906\n",
            "Epoch113/200, Loss: 0.428, Accuracy0.875\n",
            "Epoch114/200, Loss: 0.445, Accuracy0.844\n",
            "Epoch115/200, Loss: 0.398, Accuracy0.875\n",
            "Epoch116/200, Loss: 0.446, Accuracy0.781\n",
            "Epoch117/200, Loss: 0.619, Accuracy0.750\n",
            "Epoch118/200, Loss: 0.450, Accuracy0.844\n",
            "Epoch119/200, Loss: 0.340, Accuracy0.844\n",
            "Epoch120/200, Loss: 0.620, Accuracy0.688\n",
            "Epoch121/200, Loss: 0.284, Accuracy0.938\n",
            "Epoch122/200, Loss: 0.448, Accuracy0.781\n",
            "Epoch123/200, Loss: 0.439, Accuracy0.812\n",
            "Epoch124/200, Loss: 0.479, Accuracy0.719\n",
            "Epoch125/200, Loss: 0.334, Accuracy0.875\n",
            "Epoch126/200, Loss: 0.431, Accuracy0.875\n",
            "Epoch127/200, Loss: 0.494, Accuracy0.844\n",
            "Epoch128/200, Loss: 0.260, Accuracy0.938\n",
            "Epoch129/200, Loss: 0.374, Accuracy0.844\n",
            "Epoch130/200, Loss: 0.421, Accuracy0.844\n",
            "Epoch131/200, Loss: 0.355, Accuracy0.844\n",
            "Epoch132/200, Loss: 0.468, Accuracy0.812\n",
            "Epoch133/200, Loss: 0.516, Accuracy0.750\n",
            "Epoch134/200, Loss: 0.272, Accuracy0.938\n",
            "Epoch135/200, Loss: 0.378, Accuracy0.875\n",
            "Epoch136/200, Loss: 0.424, Accuracy0.812\n",
            "Epoch137/200, Loss: 0.341, Accuracy0.844\n",
            "Epoch138/200, Loss: 0.712, Accuracy0.656\n",
            "Epoch139/200, Loss: 0.439, Accuracy0.844\n",
            "Epoch140/200, Loss: 0.205, Accuracy0.969\n",
            "Epoch141/200, Loss: 0.268, Accuracy0.938\n",
            "Epoch142/200, Loss: 0.251, Accuracy0.906\n",
            "Epoch143/200, Loss: 0.468, Accuracy0.875\n",
            "Epoch144/200, Loss: 0.291, Accuracy0.938\n",
            "Epoch145/200, Loss: 0.461, Accuracy0.781\n",
            "Epoch146/200, Loss: 0.335, Accuracy0.875\n",
            "Epoch147/200, Loss: 0.467, Accuracy0.781\n",
            "Epoch148/200, Loss: 0.528, Accuracy0.750\n",
            "Epoch149/200, Loss: 0.383, Accuracy0.875\n",
            "Epoch150/200, Loss: 0.578, Accuracy0.750\n",
            "Epoch151/200, Loss: 0.346, Accuracy0.812\n",
            "Epoch152/200, Loss: 0.393, Accuracy0.844\n",
            "Epoch153/200, Loss: 0.341, Accuracy0.844\n",
            "Epoch154/200, Loss: 0.502, Accuracy0.688\n",
            "Epoch155/200, Loss: 0.604, Accuracy0.750\n",
            "Epoch156/200, Loss: 0.296, Accuracy0.875\n",
            "Epoch157/200, Loss: 0.259, Accuracy0.906\n",
            "Epoch158/200, Loss: 0.433, Accuracy0.812\n",
            "Epoch159/200, Loss: 0.319, Accuracy0.875\n",
            "Epoch160/200, Loss: 0.585, Accuracy0.781\n",
            "Epoch161/200, Loss: 0.351, Accuracy0.875\n",
            "Epoch162/200, Loss: 0.497, Accuracy0.781\n",
            "Epoch163/200, Loss: 0.433, Accuracy0.844\n",
            "Epoch164/200, Loss: 0.396, Accuracy0.812\n",
            "Epoch165/200, Loss: 0.580, Accuracy0.781\n",
            "Epoch166/200, Loss: 0.613, Accuracy0.750\n",
            "Epoch167/200, Loss: 0.485, Accuracy0.781\n",
            "Epoch168/200, Loss: 0.266, Accuracy0.938\n",
            "Epoch169/200, Loss: 0.500, Accuracy0.781\n",
            "Epoch170/200, Loss: 0.397, Accuracy0.812\n",
            "Epoch171/200, Loss: 0.296, Accuracy0.906\n",
            "Epoch172/200, Loss: 0.203, Accuracy0.969\n",
            "Epoch173/200, Loss: 0.547, Accuracy0.812\n",
            "Epoch174/200, Loss: 0.341, Accuracy0.875\n",
            "Epoch175/200, Loss: 0.393, Accuracy0.844\n",
            "Epoch176/200, Loss: 0.412, Accuracy0.844\n",
            "Epoch177/200, Loss: 0.450, Accuracy0.844\n",
            "Epoch178/200, Loss: 0.402, Accuracy0.844\n",
            "Epoch179/200, Loss: 0.383, Accuracy0.875\n",
            "Epoch180/200, Loss: 0.549, Accuracy0.781\n",
            "Epoch181/200, Loss: 0.296, Accuracy0.875\n",
            "Epoch182/200, Loss: 0.398, Accuracy0.844\n",
            "Epoch183/200, Loss: 0.369, Accuracy0.844\n",
            "Epoch184/200, Loss: 0.251, Accuracy0.906\n",
            "Epoch185/200, Loss: 0.379, Accuracy0.812\n",
            "Epoch186/200, Loss: 0.781, Accuracy0.594\n",
            "Epoch187/200, Loss: 0.516, Accuracy0.812\n",
            "Epoch188/200, Loss: 0.390, Accuracy0.812\n",
            "Epoch189/200, Loss: 0.442, Accuracy0.844\n",
            "Epoch190/200, Loss: 0.263, Accuracy0.906\n",
            "Epoch191/200, Loss: 0.414, Accuracy0.781\n",
            "Epoch192/200, Loss: 0.378, Accuracy0.812\n",
            "Epoch193/200, Loss: 0.418, Accuracy0.812\n",
            "Epoch194/200, Loss: 0.315, Accuracy0.906\n",
            "Epoch195/200, Loss: 0.314, Accuracy0.844\n",
            "Epoch196/200, Loss: 0.277, Accuracy0.938\n",
            "Epoch197/200, Loss: 0.537, Accuracy0.688\n",
            "Epoch198/200, Loss: 0.316, Accuracy0.875\n",
            "Epoch199/200, Loss: 0.435, Accuracy0.812\n",
            "Epoch200/200, Loss: 0.354, Accuracy0.875\n"
          ]
        }
      ]
    }
  ]
}